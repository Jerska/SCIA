# Estimation en statistique

Le problème central de l'inférence statistique est le suivant : disposant d'observations sur un échantillon de taille $n$ $(X\_i,...,X\_n)$, $X\_i$ V.A. indépendantes et de même loi.
On désire le dé claire la propriété de la population dont il est issu.
Ainsi, on cherchera à estimer par exemple la moyenne $m$ de la population à partir de la moyenne $\overline{X}$ d'un échantillon.
Ceci n'est possible que si l'échantillon a été tiré selon des règles rigoureuses destinées à en assurer la représentativité. (Tirages équiprobables et indépendants)

#### Définition
>Une statistique $T$ est une variable aléatoire mesurable de $X\_1, X\_2, ..., X\_n$
>$$ T = f(X\_1, X\_2, ..., X\_n)$$

#### Définition
>La statistique $\overline{X}$ en moyenne empirique de l'échantillon $(X\_1, X\_2, ... , X\_n)$ est
>$$ \overline{X} = {1 \over n} \sum\limits\_{i = 1}^n X\_i $$

#### Espérance
>L'espérance de $(X\_1, X\_2, ... , X\_n)$ échantillon de $X$ s'exprime sous cette forme :
>$$ E(\overline{X}) = {1 \over n} \sum\limits\_{i=1}^{n}E(X\_i) = {\not{n}m \over \not{n}} = m = E(X)$$

#### Variance
>$$ V(\overline{X}) = {1 \over n^2} V(\sum\limits\_{i = 1}^n X\_i) =  {1 \over n^2} \sum\limits\_{i = 1}^n V(X\_i) = {\not{n}\sigma^2 \over n^{\not{2}}} = {\sigma^2 \over n} $$
>Or, $ V(X\_i) = \sigma^2 = V(X) $, d'où
$$ \lim\limits\_{n \mapsto +\infty} V(\overline{X}) = 0 $$

#### Convergence
>D'après *Chebychev*,
>$$\begin{array}{rl}
>\forall \varepsilon > 0, & P(|\overline{X} - E(\overline{X})| > \varepsilon) < {V(\overline{X}) \over \varepsilon²} \\\\
>\iff & 0 \leq P(|\overline{X} - m| > \varepsilon) < {\sigma^2 \over n \varepsilon^2} \underset{\;\;n \mapsto + \infty\;\;}{\xrightarrow{}} 0\\\\
>\implies & \lim\limits\_{n \mapsto +\infty} P(|\overline{X} - m| > \varepsilon) = 0 \\\\
>\implies & \overline{X} \overset{P}{\underset{\;\;n \mapsto + \infty\;\;}{\xrightarrow{}}} m = E(X)
>\end{array}$$
>
>On a donc convergence en probabilité vers $m$.
>On dit que **$\overline{X}$ est une estimation de la moyenne $m$**.

#### Définition
>La statistique $S²$ ou variance empirique d'un échantillon :
>$$S^2 = {1 \over n} \sum\limits\_{i = 1}^n (X\_i - \overline{X})^2$$

#### Propriété
>$$ S² = {1 \over n} \sum\limits\_{i = 1}^n X\_i² - \overline{X}² $$

** Preuve **
$$\begin{array}{rl}
S^2 = & {1 \over n} \sum\limits\_{i = 1}^n (X\_i^2 - 2\overline{X}X\_i + \overline{X}^2) \\\\
= & {1 \over n} \sum\limits\_{i = 1}^n X\_i^2 - {2\overline{X} \over n} \sum\limits\_{i = 1}^n X\_i + \overline{X}^2 \\\\
= & {1 \over n} \sum\limits\_{i = 1}^n X\_i^2 - 2\overline{X}² + \overline{X}^2 \\\\
= & {1 \over n} \sum\limits\_{i = 1}^n X\_i^2 - \overline{X}^2 \\\\
\text{D'après la loi des grands nombres, } \overline{X} = & {1 \over n} \sum\limits\_{i = 1}^n X\_i \overset{P}{\underset{\;\;n \mapsto + \infty\;\;}{\xrightarrow{}}} m = E(X\_i) = E(X) \\\\
\implies & {1 \over n} \sum\limits\_{i = 1}^n X\_i^2 \overset{P}{\underset{\;\;n \mapsto + \infty\;\;}{\xrightarrow{}}} E(X^2) \\\\
\implies & S^2 \overset{P}{\underset{\;\;n \mapsto + \infty\;\;}{\xrightarrow{}}} E(X^2) - E^2(X) = \sigma^2 = V(X)
\end{array}$$

> $$ S^2 \overset{P}{\underset{\;\;n \mapsto + \infty\;\;}{\xrightarrow{}}} \sigma^2 = V(X) $$

#### Proposition
> $$ S^2 = {1 \over n} \sum\limits\_{i = 1}^n (X\_i - m)^2 - \left(\overline{X} - m\right)^2$$

** Preuve **
$$\begin{array}{rl}
\sum\limits\_{i = 1}^n \left(X\_i - m\right)^2 = & \sum\limits\_{i = 1}^n \left(X\_i - \overline{X} + \overline{X} - m\right)^2 \\\\
= & \sum\limits\_{i = 1}^n \left(X\_i - \overline{X}\right)^2 + 2\left(\overline{X} - m\right) \sum\limits\_{i = 1}^n \left(X\_i - \overline{X}\right) + n \left(\overline{X} - m\right)^2 \\\\
{1 \over n} \sum\limits\_{i = 1}^n \left(X\_i - m\right)^2 = & {1 \over n} \sum\limits\_{i = 1}^n \left(X\_i - \overline{X}\right)^2 + {2\left(\overline{X} - m\right) \over n} \sum\limits\_{i = 1}^n \left(X\_i - \overline{X}\right) + \left(\overline{X} - m\right)^2 \;\;\;\; \left[\text{Or } \;\; {1 \over n} \sum\limits\_{i = 1}^n \left(X\_i - \overline{X}\right) = {1 \over n} \sum\limits\_{i = 1}^n X\_i - \overline{X} = \overline{X} - \overline{X} = 0 \right]\\\\
= & S^2 + \left(\overline{X} - m\right)^2\\\\
\iff S^2 = & {1 \over n} \sum\limits\_{i = 1}^n \left(X\_i - m\right)^² - \left(\overline{X} - m\right)^2
\end{array}$$

#### Conséquence

> $$ E(S^2) = {n - 1 \over n} \sigma^2 $$

** Preuve **
$$ \begin{array}{rl}
E(S^2) = & {1 \over n} \sum\limits\_{i = 1}^n E\left(\left(X\_i - m\right)^2\right) - E\left(\left(\overline{X} - m\right)^2\right) \;\;\;\; \left[\text{Or } \;\; m = E(X\_i) = E\left( \overline{X} \right) \right]\\\\
= & {1 \over n} \sum\limits\_{i = 1}^n V\left(X\_i\right) - V\left(\overline{X}\right) \\\\
= & {n \sigma^2 \over n } - {\sigma^2 \over n} \\\\
= & {n-1 \over n} \sigma^2
\end{array} $$

#### Définition
> On dit que $S^2$ est un estimateur avec biais de $\sigma^2$ si
>$$E(S^2) \neq \sigma^2$$

#### Définition
>Soit $T$ un estimateur quelconque de $\theta$ (paramètre inconnu)
>$T$ est sans biais ssi
>$$ E(T) = \theta$$

** Exemple **
$$ \overline{X} = {1 \over n} \sum\limits\_{i = 1}^n X\_i$$
$$E(\overline{X}) = m$$
$\overline {X}$ est un *estimateur sans biais*.

** Autre exemple **
Prenons $ S^{2 \star} = {n \over n - 1} S^2$
$$ E \left(S^{2 \star}\right) = {n \over n - 1} E\left(S^2\right) = {n \over n - 1} . {n - 1 \over n} \sigma^2 = \sigma^2 $$
$S^{2\star}$ est un *estimateur sans biais*.

#### Définition

>Soit $\theta$ le paramètre à estimer et $T$ un estimateur de $\theta$.
>L'erreur d'estimation se caractérise comme suit :
>$$ T - \theta = T - E(T) + E(T) - \theta $$
>$ T - E(T) $ représente les fluctuations aléatoires de $T$ autour de la moyenne.
>$ E(T) - \theta $ représente le biais.

#### Définition

>On mesure la précision d'un estimateur $T$ par son erreur quadratique $E\left(\left(T - \theta\right)^2\right)$.
$$\begin{array}{rl}
E\left(\left(T - \theta\right)^2\right) = & E\left(\left(T - E(T) + E(T) - \theta\right)^2\right) \\\\
= & E\left(\left(T - E(T)\right)^2\right) + 2E\Big(\left(T-E(T)\right)\left(E(T) - \theta\right)\Big) + E\left(\left(E(T) - \theta\right)^2\right) \\\\
= & V(T) + 2\Big(E(T) - \theta\Big)\Big(E(T) - E(E(T))\Big) + (E(T) - \theta)^2 \\\\
= & V(T) + (E(T) - \theta)^2
\end{array}$$

#### Remarque
> De 2 estimateurs sans biais de $\theta$, le plus précis est donc celui de variance minimale.

# Vraisemblance d'un échantillon

Le modèle utilisé en théorie de l'estimation est le suivant :
> On observe un échantillon $(X\_1, X\_2, ... , X\_n)$ d'une variable aléatoire $X$ dont on connaît la loi de probabilité à l'exception d'un paramètre inconnu $\theta$.

#### Définition
> La densité de l'échantillon $(X\_1, X\_2, ... , X\_n)$ est la vraisemblance de cet échantillon.

> La loi conjointe s'exprime sous cette forme (chaque $X\_i$ est dépendant de $\theta$) dans le cas discret :
> $$L(x\_1, x\_2, ..., x\_n, \theta) = P\left[(X\_1 = x\_1) \cap (X\_2 = x\_2) \cap ... \cap (X\_n = x\_n)\right]$$
> Soit la densité de l'échantillon (si $X$ est continu) :
> $$ L(x\_1, ... , x\_n) = \prod\limits\_{i = 1}^{n} f(x\_i, \theta) $$

** Exemple **
$$ X \sim \mathcal{P}(\theta)$$
Loi de Poisson de paramètre inconnu $\theta$.
$(X\_1, ..., X\_n)$ un echantillon de $X$.
La vraisemblance de $(X\_1, ... , X\_n)$ est :
$$\begin{array}{rl}
L(x\_1, ... , x\_n, \theta) = & P\left[(X\_1 = x\_1) \cap ... \cap (X\_n = x\_n) \right] \\\\
= & \prod\limits\_{i = 1}^{n} P[X\_i = x\_i] \\\\
= & \prod\limits\_{i = 1}^{n} e^{-\theta} {\theta^{x\_i} \over x\_i!} \\\\
= & {e^{-n\theta} \theta^{\sum\limits\_{i = 1}^{n} x\_i} \over \prod\limits\_{i = 1}^{n} x\_i!}
\end{array}$$

## L'information de Fisher

#### Définition
> On appelle quantité d'information de Fisher $I\_n(\theta)$ apportée par un échantillon $(X\_1, ..., X\_n)$ sur le paramètre $\theta$ :
> $$ I\_n(\theta) = E\left( \left( {\partial{\ln L} \over \partial{\theta}} \right)^2\right) $$

#### Théorème
> Si le domaine de définition de $X$ ne dépend pas de $\theta$, alors :
> $$ I\_n(\theta) = E\left({\partial^2{\ln L} \over \partial{\theta^2}}\right) $$

# L'estimation sans biais de variance minimale

#### Théorème

> S'il existe un estimateur de $\theta$ **sans biais**, alors il est **unique**.

** Preuve **
Supposons $\exists \; T\_1, T\_2$ sans biais de $\theta$ de variance minimale $V$.
Soit $T\_3 = {T\_1 + T\_2 \over 2}$
$$ E(T\_3) = {E(T\_1) + E(T\_2) \over 2} = {2 \theta \over 2} = \theta$$
$T\_3$ est sans biais.


$$\begin{array}{rl}
V(T\_3) = & {1 \over 4} V(T\_1 + T\_2) = {1 \over 4}\Big(V(T\_1) + V(T\_2) + 2\mathrm{Cov}(T\_1, T\_2)\Big) \;\;\; \left[ \text{N.B. : Covariance : }\mathrm{Cov}(T\_1, T\_2) = \rho \sigma\_1 \sigma\_2 \iff \rho = {\mathrm{Cov} (T\_1, T\_2) \over \sigma\_1 \sigma\_2} \right] \\\\
= & {1 \over 4} \left( V(T\_1) + V(T\_2) + 2 \rho \sigma\_1 \sigma\_2 \right) = {1 \over 4} (V + V + 2\rho V) \\\\
= & {V \over 2}(1 + \rho) \;\;\;\; \Big[ \text{Si } \rho < 1 \text{, on a } V(T\_3) < V \text{ impossible !} \implies \rho = 1 \Big] \\\\
\implies & T\_1 - E(T\_1) = \lambda\Big(T\_2 - E(T\_2)\Big) \\\\
\iff & T\_1 - \theta = \lambda(T\_2 - \theta) \;\;\;\; \Big[ \text{Or } \;\; V(T\_1) = V(T\_2) = V \Big] \\\\
\implies & \lambda = 1 \\\\
\implies & T\_1 = T\_2
\end{array}$$
